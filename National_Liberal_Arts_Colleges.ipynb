{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2c121bc-2e87-4584-ba72-393f5682051b",
   "metadata": {},
   "source": [
    "# National Liberal Arts Colleges Data\n",
    "\n",
    "- **Author:** Cameron Raymond\n",
    "- **Affliation:** University of Oxford, Oxford Internet Institute\n",
    "- **Program:** MSc, Social Data Science\n",
    "- **Email:** [cameron.raymond@hey.com](mailto:cameron.raymond@hey.com)\n",
    "\n",
    "Scraping the 2021 rankings from the National Liberal Arts Colleges rankings: https://www.usnews.com/best-colleges/rankings/national-liberal-arts-colleges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9bed49e-89ef-49e8-abff-c38aed8ff7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Common webscraping libaries\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d218953e-2ba1-412c-92fe-2f67f9a31cbf",
   "metadata": {},
   "source": [
    "# Get list of universities\n",
    "\n",
    "I noticed that when you clicked the \"load more\" button on the original webpage, it made a call to a public API. Click [here](https://www.usnews.com/best-colleges/api/search?_sort=rank&_sortDirection=asc&_page=1&schoolType=national-universities) to see the JSON structure (relevant data is under the `data/items` field). This returned a lot of relevant data in a JSON format. Webscrapers are notoriously brittle (ie they often break if the webpage changes) so getting as much data from the source is preferable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12d64afa-5f3b-4aab-81b8-322ee816b2ac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Ranking</th>\n",
       "      <th>info_page</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Whittier College</td>\n",
       "      <td>#102</td>\n",
       "      <td>whittier-college-1342</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hope College</td>\n",
       "      <td>#112</td>\n",
       "      <td>hope-college-2273</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Drew University</td>\n",
       "      <td>#113</td>\n",
       "      <td>drew-university-2603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Elizabethtown College</td>\n",
       "      <td>#113</td>\n",
       "      <td>elizabethtown-college-3262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Grove City College</td>\n",
       "      <td>#113</td>\n",
       "      <td>grove-city-college-3269</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>218</th>\n",
       "      <td>Luther College</td>\n",
       "      <td>#102</td>\n",
       "      <td>luther-college-1874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>219</th>\n",
       "      <td>Randolph-Macon College</td>\n",
       "      <td>#102</td>\n",
       "      <td>randolph-macon-college-3733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>220</th>\n",
       "      <td>Saint Anselm College</td>\n",
       "      <td>#102</td>\n",
       "      <td>st-anselm-college-2587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>221</th>\n",
       "      <td>Southwestern University</td>\n",
       "      <td>#102</td>\n",
       "      <td>southwestern-university-3620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>222</th>\n",
       "      <td>St. John's University (MN)</td>\n",
       "      <td>#102</td>\n",
       "      <td>st-johns-university-minnesota-2379</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>223 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Name Ranking                           info_page\n",
       "0              Whittier College    #102               whittier-college-1342\n",
       "1                  Hope College    #112                   hope-college-2273\n",
       "2               Drew University    #113                drew-university-2603\n",
       "3         Elizabethtown College    #113          elizabethtown-college-3262\n",
       "4            Grove City College    #113             grove-city-college-3269\n",
       "..                          ...     ...                                 ...\n",
       "218              Luther College    #102                 luther-college-1874\n",
       "219      Randolph-Macon College    #102         randolph-macon-college-3733\n",
       "220        Saint Anselm College    #102              st-anselm-college-2587\n",
       "221     Southwestern University    #102        southwestern-university-3620\n",
       "222  St. John's University (MN)    #102  st-johns-university-minnesota-2379\n",
       "\n",
       "[223 rows x 3 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "from glob import glob\n",
    "\n",
    "def parse_item(university):\n",
    "    \"\"\"\n",
    "        Each university is represented by a dictionary, this function parses out the relevant features\n",
    "    \"\"\"\n",
    "    name = university['institution']['displayName']\n",
    "    ranking = university['institution']['rankingDisplayRank']\n",
    "    page_url = f\"{university['institution']['urlName']}-{university['institution']['primaryKey']}\"\n",
    "    return {\"Name\": name,\"Ranking\":ranking,\"info_page\": page_url}\n",
    "\n",
    "def get_data(fp):\n",
    "    with open(fp) as file:\n",
    "        json_data = json.load(file)\n",
    "    return json_data['data']['items']\n",
    "\n",
    "\n",
    "records = []\n",
    "fps = glob(\"data/raw/page-*.json\")\n",
    "for fp in fps:\n",
    "    page_data = get_data(fp)\n",
    "    record_subset = map(parse_item,page_data)\n",
    "    records += record_subset\n",
    "\n",
    "uni_df = pd.DataFrame.from_records(records)\n",
    "uni_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9a3e51-cf42-4c60-a112-c79c36382591",
   "metadata": {},
   "source": [
    "# Parse information from website profiles\n",
    "\n",
    "Since the API doesn't have a publically available endpoint that returns the website profile data in a JSON format I'll scrape the html page and parse info from that directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "994cfedc-dfcb-449e-aba1-7fdc7f4d00a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "headers = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:88.0) Gecko/20100101 Firefox/88.0',\n",
    "    'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',\n",
    "    'Accept-Language': 'en-CA,en-US;q=0.7,en;q=0.3',\n",
    "    'DNT': '1',\n",
    "    'Connection': 'keep-alive',\n",
    "    'Upgrade-Insecure-Requests': '1',\n",
    "    'Cache-Control': 'max-age=0',\n",
    "}\n",
    "\n",
    "def parse_info_page(info_page):\n",
    "    time.sleep(1)\n",
    "#     The info page parameter gives the url suffix    \n",
    "    url = f\"https://www.usnews.com/best-colleges/{info_page}\"\n",
    "    r = requests.get(url,headers = headers)\n",
    "    print(r)\n",
    "#     Beautifulsoup provides functionality to manipulate HTML data.\n",
    "    soup = bs(r.text)\n",
    "    print(soup)\n",
    "    try:\n",
    "            overview = get_overview(soup)\n",
    "    except Exception as e:\n",
    "        print(info_page)\n",
    "        print(e)\n",
    "        \n",
    "    try:\n",
    "        general_information = get_general_information(soup)\n",
    "    except Exception as e:\n",
    "        print(info_page)\n",
    "        print(e)\n",
    "#     Merge the two sets of information into one dictionary\n",
    "    page_info = overview | general_information\n",
    "    return pd.Series(page_info)\n",
    "\n",
    "def get_overview(soup):\n",
    "#     mb5 is a big div with the overview text, but it also contains a lot of junk/random html components\n",
    "    overview = soup.findAll(\"div\",{\"class\":\"Raw-slyvem-0 util__RawContent-sc-1kd04gx-2 hEdtf imFNkN\"})[0]\n",
    "#     Find all the text tags (drop the last one as it is an advertisement)\n",
    "    overview = overview.findAll(\"p\")\n",
    "#     Use the join command to put it into one string. Use the lambda function to get rid of the html tags and\n",
    "#     only keep the text within the <p> </p> tags.\n",
    "    overview = \" \".join(map(lambda x : x.text,overview))\n",
    "    return {\"Overview\": overview}\n",
    "\n",
    "def get_general_information(soup):\n",
    "    info_dict = {}\n",
    "#     The general information section is a div with the class below. It contains multiple other divs which\n",
    "#     represent the various rows. First we find the container div, and then we break that up into a list of the\n",
    "#     remaining rows\n",
    "    general_information = soup.find(\"div\",{\"class\":\"Cell-sc-1abjmm4-0 dsivYq\"}).findAll(\"div\")\n",
    "#     We leave out the last element (the school website) as it has a different structure\n",
    "    for div in general_information:\n",
    "#         Each row has two <p> tags, the first represents the type of info, \n",
    "#         and the second represents the value\n",
    "        try:\n",
    "            key, value = div.findAll(\"p\")\n",
    "            info_dict[key.text] = value.text\n",
    "        except:\n",
    "            print(div.findAll(\"p\"))\n",
    "    return info_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b3a9b75-c5cd-46cb-98c9-7c7547474cf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now that we've written the functions that parse a single webpage, we go through each row in our original\n",
    "# dataframe, take the \"info_page\" column (which indicates the url that the info page can be found at)\n",
    "# and use the functions from the preceding cell to parse out that relevant info and add it to the dataframe\n",
    "uni_df[[\"Overview\",\n",
    "        \"School Type\",\n",
    "        \"Year Founded\",\n",
    "        \"Religious Affiliation\",\n",
    "        \"Academic Calendar\",\n",
    "        \"Setting\",\n",
    "        \"2019 Endowment\",\n",
    "        \"School Website\"]] = uni_df[\"info_page\"].apply(parse_info_page)\n",
    "# Drop the info_page parameter as it wasn't requested\n",
    "uni_df = uni_df.drop(\"info_page\",axis=1)\n",
    "uni_df.index.name = \"Index\"\n",
    "uni_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec533bfc-4823-426b-9475-96ffbc7c5804",
   "metadata": {},
   "source": [
    "It is important to check if any rows have slipped through the cracks when rangling, or if there is an error in the code's logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659d53b4-54fb-4667-8cd9-2628e36c7301",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check to see if there are any rows with missing data\n",
    "assert len(uni_df[uni_df.isnull().any(axis=1)]) == 0, \"Empty!\"\n",
    "print(\"All rows have data!\")\n",
    "# I picked a random university from the website and checked to make sure that it had the correct ranking in my data\n",
    "ball_state = uni_df[uni_df[\"Name\"] == \"Ball State University\"]\n",
    "assert (ball_state[\"Ranking\"] == \"#284\").all(), \"Wrong ranking for Ball State University\"\n",
    "print(\"Correct ranking for Ball State University!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b3234c3-f885-4017-b779-a795c7ddc5f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data to CSV.\n",
    "uni_df.to_csv(\"usnews-college_info.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfa65595-510a-450d-90e2-e1542ff944d2",
   "metadata": {},
   "source": [
    "# Additional parsing\n",
    "\n",
    "This wasn't specified, but numeric fields like `ranking` and `endowment` need to be parsed as numbers. This poses some problems. First, the lower ranked universities are given by a range rather than a specific ranking (i.e. \\#289-389). I'll use the upper bound of the ranking in this case. Second, the endowment column doesn't use a common unit (i.e millions, billions, etc.). This will require some logic to handle. After doing so, however, the data will be easier to visualize and work with in regression frameworks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac8e99d2-1b0c-453d-adff-53d53f29fe8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_df = pd.read_csv('usnews-college_info.csv',index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eace70a-f87c-47d5-ac8c-f8932a741a84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes the `ranking` column and first removes the first digit (a # symbol) with .str[1:]\n",
    "# then it splits that string into a list delineaed by the hyphen ('289-389' -> ['289','389'])\n",
    "# and takes the first element from that list (.str[0]). Then I convert it to an integer.\n",
    "uni_df['Ranking'] = uni_df['Ranking'].str[1:].str.split('-').str[0].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d3200f-5fbf-4b87-9eb8-23bc2b79ea9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "multipliers = {\n",
    "    'million': 1000000,\n",
    "    'billion': 1000000000\n",
    "}\n",
    "\n",
    "def parse_endowment(end_str):\n",
    "    # If the argument isn't a string return what it was\n",
    "    if type(end_str) is not str:\n",
    "        return end_str\n",
    "    split = end_str.split()\n",
    "    # If we split the endowment string and it is a list with only one element, then that means the\n",
    "    # endowment wasn't reported, so return NaN\n",
    "    if len(split) < 2:\n",
    "        return np.nan\n",
    "    try:\n",
    "        # Third element is normally a '+' symbol, so we'll take the first two elements\n",
    "        number,mult = split[:2]\n",
    "        # Convert the number to a number type (1st symbol is dollar sign so remove that)\n",
    "        number = float(number[1:]) \n",
    "        mult = multipliers[mult.lower()]\n",
    "        return number * mult\n",
    "    except:\n",
    "        # If for whatever reason the above code fails to run notify and return None\n",
    "        # I use None rather than NaN so that problematic rows can be isolated and inspected\n",
    "        print(\"Parsing failed\")\n",
    "        return None\n",
    "    \n",
    "for x in ['$124.9 million +',np.nan,'NaN','nan','N/A','$40.9 billion +']:\n",
    "    print(f\"\\\"{x}\\\" gets parsed as {parse_endowment(x)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0cf5e3-3799-4000-973d-d9fd5848a67a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the above function to our dataframe\n",
    "uni_df['2019 Endowment'] = uni_df['2019 Endowment'].apply(parse_endowment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "309d0da9-fb0b-41cd-9e22-7e221b398bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the numeric data to CSV.\n",
    "uni_df.to_csv(\"usnews-college_info-numeric.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb84315e-b387-41af-9252-7eac8846e6e5",
   "metadata": {},
   "source": [
    "Now that the relevant numeric columns are parsed as such it is easy to do some basic visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "528f4434-77b5-4b1d-9714-188c663bb4bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common graphing libraries\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "sns.set_style(\"whitegrid\",{'axes.spines.left' : False,\n",
    "                           'axes.spines.right': False,\n",
    "                           'axes.spines.top': False,\n",
    "                           'grid.linestyle': ':'})\n",
    "sns.set_context(\"talk\")\n",
    "\n",
    "g = sns.scatterplot(data=uni_df,x='2019 Endowment',y='Ranking',hue=\"Setting\")\n",
    "g.invert_yaxis()\n",
    "g.set(xscale='log',title=\"US News ranking as a function of endowment.\")\n",
    "plt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e0a805-1275-4211-a22f-1dec07313352",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (NLAC)",
   "language": "python",
   "name": "nlac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "title": "Princeton Web Scraping Task",
  "toc-autonumbering": false,
  "toc-showcode": false,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
